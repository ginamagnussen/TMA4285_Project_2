---
title: "TMA4285 Time series models"
subtitle: "Exercise 8: Comparison of a state space model and a SARIMA model"
author: "Sivert Selnes, Kristine L. Mathisen, Gina Magnussen"
date: "20th of October 2018"
output: 
  pdf_document:
    #fig_caption: true
    #toc: true
    
#header-includes:
#    - \usepackage{caption}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}
# Libraries
library(astsa)
library(forecast)
library(ggplot2)
library(dlm)
library(KFAS)
library(zoo)
library(boot)
# libraries for state-space: dse, dlm, KFAS, stsm
```

#### Remember:

* Note how uncertainty should be written: 0.056(7) (See sources on web page)


## Title
## Abstract 
## Introduction
## Theory
```{r, child='theory.Rmd'}
```

---------------------------------------------------------

## Data analysis


## 1. Exploring the data with plotting of relevant statistics

## 2. Justification of the choice of model

## 3. Model parameter estimation including uncertainty 
<!-- Simulation -->
## 4. Model prediction at the given sample points and for the next year including uncertainty of the best linear predictions
<!-- Forecasting -->
## 5. Diagnostics, and model choice discussion including comparison of the two models


## SARIMA$(p,d,q)\times(P,D,Q)_s$ model

The data set consists of total number of international airline passengers, in thousands, for each month from January 1949 to December 1960 (reference to data set), giving a total number of $N=144$ observations. 


```{r, include=TRUE}
dataseries <- ts(read.table("dataEx8.txt"), frequency = 12)
par(mfrow=c(2,2))
plot(dataseries,main = "Airline passengers (1949-1960)", ylab="Passengers (in thousands)")
boxplot(dataseries~cycle(dataseries), xlab="Date", ylab = "Passengers (in thousands)" ,main ="Monthly international air passengers") # MÃ¥ ikke tas med, men viser litt av strukturen til dataene.
acf(dataseries, lag.max = 50)
pacf(dataseries, lag.max = 50)
```

From the time series plot, this is obviously a non-stationary series as both trend and seasonality is visible in the visualization. Trend is there because the total number of airline passengers on average increases for each month, and seasonality because of the wave-like behaviour of the series. Non-stationarity is further supported by looking at the autocorrelation function, which shows considerable correlation between observations even up to lag $h$ of size $30$, and again a periodic wave-like pattern. Furthermore, the box plot shows both higher mean number of passengers and also higher variance for months 6 to 9 in the year, i.e. June til September.

To investigate the data, we first find a transformation to the time series. A typical transformation for making the series more stationary is the $\log()$-transformation. This seems to stabilize the multiplicative behaviour of the variance so that the variance is not increasing with time.

```{r}
# Log-transform the data
log_series <- log(dataseries)
# Differencing the data
diff_series <- diff(log_series)
diff12_series <- diff(diff_series, lag = 12)

# Plotting the transformed data series
plot.ts(cbind(log_series, diff_series, diff12_series), main="Transformed and differenced data")
```

Secondly, the data is differenced to remove trend, i.e. stabilizing the mean. The parameter $d$ is related to trend within a season, and since this trend seems to be removed by differencing once, we try $d = 1$. See the plot in the middle of the figure for transformed and differenced data. After differencing there still is a wave pattern present, one for each year. This indicates a seasonal trend equal to the length of a year, i.e. we suspect a $s = 12$ in a seasonal ARIMA model. Then $(1-B^{12})$ is applied to th series. The parameter $D$ is related to the trend between seasons, i.e. the trend one can se from one time of the year to the next, and the next, and so on. We therefore set $D = 1$. Differencing once indicates linear trend, both within and between the seasons. 

Our model parameter choices can be checked by comparing to the decomposed time series:
```{r}
autoplot(decompose(dataseries, "multiplicative"))
```
which clearly shows linear trend and a season equal to a year.

```{r}
ggtsdisplay(diff12_series)
```

The obtained series after applying the twelfth-order difference AGAIN: PROBABLY NOT RIGHT seems stationary without any trend or seasonality. We can then estimate the remaining parameters by considering the ACF and the PACF. This could be done by looking at lags equal to $1s, 2s,...$, $s=12$ to determine $P$ and $Q$ in the seasonal component and by looking at smaller lags in each season to determine $p$ and $q$ in the non-seasonal component.  In this case, however, we have used the AICC criterion and tested models with $d=1$, $D=1$, $s=12$, which is determined before, and $p,q,P,Q < 5$ which is based by looking at the ACF and PACF. In addition we prefer models with smaller parameters for simplicity if they are a good fit.

```{r}
bestfit <- auto.arima(log_series, d=1, D=1, max.p=5,max.q=5, max.P=5, max.Q=5, seasonal=TRUE, ic="aicc", trace=T)
```


```{r, include=FALSE}
fit <- sarima(log_series, 0,1,1,0,1,1,12)
fit$ttable # Gives parameter estimates and their standard deviation

# Bootstrap

n=length(log_series)
B=10
estimator=matrix(NA,nrow=2, ncol = B)
# Bootstrap sample, store parameter vector

for (b in 1:B)
  {
  boot=sample(x=log_series,size=n,replace=TRUE)
  model = sarima(boot, 0,1,1,0,1,1,12)
  estimator[,b] = model$ttable[,1]
}

# Find mean and variance of each parameter
mean = rowMeans(estimator) 
variance = rep(NA,2)
for (p in 1:2){
  variance[p] = var(estimator[p,])
}

mean
variance
```

Both parameters are significant in the model, and the residual analysis plot shows that our model is a good fit. In addition, the uncertainty of the parameters are given from $\texttt{fit\$ttable}$. Our chosen model is thus 

\[
\text{SARIMA}(0,1,1)\times(0,1,1)_{12} 
\]

which is equal to

\[
\phi(B)\Phi(B^{12})(1-B)(1-B^{12})X_t = \theta(B)\Theta(B^{12})Z_t
\]

When we have found our model, we can do forecasting with the SARIMA model for the next twelve months. From the fitting functions in R, we can also extract credible intervals of the forecasts and the fitted values at our given sample points.

```{r, include = FALSE}

# Forecasting
future <- 12 # Set no. of months to forecast

# Forecasting
forecast_sarima <- forecast(log_series, h = future) # Forecastin
log_fitted <- forecast_sarima$fitted # Fitted values for already given sample points

# Check model for given sample points
sarima_frame <- data.frame(dataseries, exp(log_fitted), log_series, log_fitted,  1:144)
colnames(sarima_frame) <- c("original", "fitted", "log_original", "log_fitted", "month")
```

The following plot shows the time series and the forecased values for the next twelve months: 

```{r}
plot(forecast_sarima)
```

The credible intervals for the forecasts are

```{r}
uncertainty <- data.frame("95percent_lower" = exp(forecast_sarima$lower)[,2], "95percent_upper"=exp(forecast_sarima$upper)[,2])
uncertainty

```


```{r}
ggplot(sarima_frame) + 
  geom_line(aes(month, dataseries, color = "original")) + 
  geom_line(aes(month,fitted , color="fitted")) + 
  labs(title="Original and fitted values for SARIMA", xlab="Months", ylab="Passengers")
```

The fitted data looks good compared to the original data and the forecasting seems to continue the season and trend of the data well. 





### State-space model

We build a state-space model by ??eqref to theory or just use Rfunctions??, which gives the following matrices

```{r, echo=FALSE}
# ap <- log10(AirPassengers) - 2
state_space <- StructTS(dataseries, type = "BSM") # Basic structural model
state_space_log <- StructTS(log_series, type ="BSM")
# state_space$model$T # F in state equation
forecasts <- forecast(state_space_log, h = 50)
#forecasts$lower 
#forecasts$upper
plot(forecasts)
#state_space$model$T # gives the F matrix
#state_space$model$Z # gives the G matrix
```
$F=$
```{r}
print(state_space$model$T)
```
$G=$
```{r}
print(state_space$model$Z)
```

$Q=$
```{r}
print(state_space$model$V)
```
This gives $(\hat\sigma_1^2, \hat\sigma_2^2, \hat\sigma^3_2) = (0.0000, 160.9755,  29.84652)$.



```{r}
print(state_space$model$a)
```


```{r}
model.build <- function(p) {
  return(
    dlmModPoly(2, dV=p[1], dW=p[2:3]) + 
      dlmModSeas(12, dV=p[4])
  )
}
 
#log.air <- log(air) + rnorm(length(log.air), 0, 0.15)
log.air <- log_series
train <- log.air[1:120]
test <- log.air[121:144]
 
model.mle <- dlmMLE(train, parm=c(1, 1, 1, 1), build=model.build)
model.fit <- model.build(model.mle$par)
model.filtered <- dlmFilter(train, model.fit)
model.smoothed <- dlmSmooth(train, model.fit)

n <- 2*12
model.forecast <- dlmForecast(model.filtered, nAhead=n)
 
x <- index(log.air)
a <- drop(model.forecast$a%*%t(FF(model.fit)))
df <- rbind(
  data.frame(x=index(log.air), y=as.numeric(log.air), series="original"),
  #data.frame(x=x[1:120], y=apply(model.filtered$m[-1,1:2], 1, sum), series="filtered"),
  #data.frame(x=x[1:120], y=apply(model.smoothed$s[-1,1:2], 1, sum), series="smoothed"),
  data.frame(x=x[121:144], y=a, series="forecast")
)
g.dlm <- ggplot(df, aes(x=x, y=y, colour=series)) + geom_line()
g.dlm
```

?? Also need to forecast for given sample points here and find uncertainty. ??

## Bootstrap
We bootstrap the estimated parameters for the state space model. 
```{r}
n=length(log_series)
B=10
estimator=matrix(NA,nrow=4, ncol = B)
# Bootstrap sample, store parameter vector
for (b in 1:B)
  {
  boot=sample(x=log_series,size=n,replace=TRUE)
  model = dlmMLE(boot, parm=c(1, 1, 1, 1), build=model.build)
  estimator[,b] = model$par
}

# Find mean and variance of each parameter
mean = rowMeans(estimator) 
variance = rep(NA,4)
for (p in 1:4){
  variance[p] = var(estimator[p,])
}

```

* Model diagnostics for state-space? I don't know. \\
* Model comparison + discussion. \\
* Theory \\
??

-------------------------------------------


## Discussion
## Conclusion
## Appendix
## References

